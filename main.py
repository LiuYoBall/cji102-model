import os
import torch
from contextlib import asynccontextmanager
from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse

# å¼•ç”¨æˆ‘å€‘å¯«å¥½çš„æ¨¡çµ„
from cfp_classify import load_cfp_model
from controller import process_fundus_image

# --- è¨­å®šèˆ‡å…¨åŸŸè®Šæ•¸ ---
# Cloud Run GCS Mount è·¯å¾‘é€šå¸¸è¨­ç‚º /mnt/gcs_bucket_name
# æœ¬åœ°æ¸¬è©¦æ™‚ï¼Œå¯æ”¹ç‚ºä½ çš„æœ¬æ©Ÿè·¯å¾‘
MODEL_MOUNT_PATH = os.getenv("MODEL_MOUNT_PATH", "/mnt/models") 
CFP_MODEL_FILENAME = "0104_RETFound_inference.pth"
YOLO_MODEL_FILENAME = "best_yolo.pt" # é ç•™

# modelæœ¬åœ°è·¯å¾‘(æ¸¬è©¦ç”¨)
local_model_path = r"C:\Users\TMP-214\Desktop\deployment\model\0104_RETFound_inference.pth"

models = {}
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- 1. Lifespan: å•Ÿå‹•æ™‚è¼‰å…¥æ¨¡å‹ (é—œéµå„ªåŒ–) ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    print(f"ğŸš€ Starting up... Device: {device}")
    
    # å»ºæ§‹å®Œæ•´è·¯å¾‘
    cfp_path = os.path.join(MODEL_MOUNT_PATH, CFP_MODEL_FILENAME)
    
    # å˜—è©¦è¼‰å…¥ CFP æ¨¡å‹
    try:
        if os.path.exists(cfp_path):
            # å„ªå…ˆå˜—è©¦ Cloud Run GCS æ›è¼‰è·¯å¾‘
            models["cfp"] = load_cfp_model(cfp_path, device)
            print(f"âœ… CFP Model loaded from GCS Mount: {cfp_path}")
        elif os.path.exists(local_model_path):
            # å…¶æ¬¡å˜—è©¦æœ¬åœ°çµ•å°è·¯å¾‘ (Local Test)
            print(f"âš ï¸ GCS Mount not found. Loading from Local Path: {local_model_path}")
            models["cfp"] = load_cfp_model(local_model_path, device)
        else:
            # å…©è€…éƒ½æ‰¾ä¸åˆ° (é¿å…ç¨‹å¼å´©æ½°ï¼Œä½†æ¨™è¨˜æœå‹™ä¸å¯ç”¨)
            print("âŒ Critical Error: No model file found in GCS or Local path.")
            models["cfp"] = None
    except Exception as e:
        print(f"âŒ Error loading CFP model: {e}")
        models["cfp"] = None

    # (é ç•™) è¼‰å…¥æ¨¡å‹
    # models["yolo"] = load_yolo_model(...)
    models["yolo"] = None
    models["oct"] = None  

    yield
    
    # é—œé–‰æ™‚æ¸…ç†
    print("ğŸ›‘ Shutting down. Clearing GPU memory...")
    models.clear()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

app = FastAPI(lifespan=lifespan)

# --- 2. Health Check (Cloud Run éœ€è¦) ---
@app.get("/")
def health_check():
    status = "ready" if models.get("cfp") is not None else "model_missing"
    return {"status": status, "device": str(device)}

# --- 3. cfp API å…¥å£ ---
@app.post("/predict/cfp")
async def predict_cfp_endpoint(
    file: UploadFile = File(...),
    # background_tasks: BackgroundTasks # è‹¥éœ€èƒŒæ™¯ä¸Šå‚³å¯å•Ÿç”¨
):
    # 1. æª¢æŸ¥æ¨¡å‹æ˜¯å¦å°±ç·’
    if models.get("cfp") is None:
        raise HTTPException(status_code=503, detail="Model not loaded")

    # 2. é©—è­‰æª”æ¡ˆæ ¼å¼
    if file.content_type not in ["image/jpeg", "image/png", "image/jpg"]:
        raise HTTPException(status_code=400, detail="Invalid file type. Only JPG/PNG supported.")

    try:
        # 3. è®€å–æª”æ¡ˆå…§å®¹
        file_bytes = await file.read()
        
        # 4. å‘¼å« Controller é€²è¡Œè™•ç†
        # æ³¨æ„ï¼šprocess_fundus_image æ˜¯ async çš„
        result = await process_fundus_image(
            file_bytes=file_bytes,
            model_cfp=models["cfp"],
            model_yolo=models["yolo"],
            device=device
        )
        
        return JSONResponse(content=result)

    except Exception as e:
        print(f"Error processing request: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    
# # --- 3. oct API å…¥å£ ---
# @app.post("/predict/oct")

if __name__ == "__main__":
    import uvicorn
    # æœ¬åœ°æ¸¬è©¦å•Ÿå‹•æŒ‡ä»¤
    uvicorn.run(app, host="0.0.0.0", port=8080)