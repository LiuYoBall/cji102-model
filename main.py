import os
import torch
import logging
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from typing import Literal, Optional

# å¼•ç”¨æˆ‘å€‘å¯«å¥½çš„æ¨¡çµ„
# controller å·²ç¶“ä¿®æ”¹ç‚ºæ¥æ”¶ gcs_path 
from cfp_classify import load_cfp_model
from controller import process_fundus_image 

# --- è¨­å®š Log ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- è¨­å®šè·¯å¾‘èˆ‡è®Šæ•¸ ---
MODEL_MOUNT_PATH = os.getenv("MODEL_MOUNT_PATH", "/mnt/models") 
CFP_MODEL_FILENAME = "0104_RETFound_inference.pth"
YOLO_MODEL_FILENAME = "best_yolo.pt" # é ç•™
OCT_MODEL_FILENAME = "oct_model.pth" # é ç•™

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- Pydantic Model: å®šç¾©è¼¸å…¥ JSON æ ¼å¼ ---
class PredictionRequest(BaseModel):
    image_gcs_path: str = Field(..., description="GCSä¸Šçš„åœ–ç‰‡è·¯å¾‘")
    request_id: Optional[str] = None

# --- 1. Lifespan: å•Ÿå‹•èˆ‡é—œé–‰ç”Ÿå‘½é€±æœŸç®¡ç† ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info(f"ğŸš€ Starting up... Device: {device}")
    
    # åˆå§‹åŒ–æ¨¡å‹å®¹å™¨
    models = {
        "cfp": None,
        "yolo": None,
        "oct": None
    }

    # å»ºæ§‹è·¯å¾‘
    cfp_path = os.path.join(MODEL_MOUNT_PATH, CFP_MODEL_FILENAME)
    # oct_path = os.path.join(MODEL_MOUNT_PATH, OCT_MODEL_FILENAME) # é ç•™

    # --- è¼‰å…¥ CFP æ¨¡å‹ ---
    try:
        if os.path.exists(cfp_path):
            models["cfp"] = load_cfp_model(cfp_path, device)
            logger.info(f"âœ… CFP Model loaded from: {cfp_path}")
        else:
            logger.error(f"âŒ Critical Error: CFP model not found at {cfp_path}")
            # åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­ï¼Œé€™è£¡å¯ä»¥é¸æ“‡æ˜¯å¦è¦ raise error é˜»æ­¢å•Ÿå‹•
    except Exception as e:
        logger.error(f"âŒ Error loading CFP model: {e}")

    # --- (é ç•™) è¼‰å…¥ YOLO æ¨¡å‹ ---
    # try:
    #     models["yolo"] = load_yolo_model(yolo_path, device)
    # except...

    # --- (é ç•™) è¼‰å…¥ OCT æ¨¡å‹ ---
    # try:
    #     if os.path.exists(oct_path):
    #          models["oct"] = load_oct_model(oct_path, device)
    # except...

    # [é—œéµ] å°‡æ¨¡å‹å­˜å…¥ app.stateï¼Œä¾›å…¨åŸŸå­˜å–
    app.state.models = models

    yield  # æ‡‰ç”¨ç¨‹å¼é‹è¡Œä¸­...

    # --- é—œé–‰æ™‚æ¸…ç† ---
    logger.info("ğŸ›‘ Shutting down. Clearing GPU memory...")
    app.state.models.clear()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

app = FastAPI(lifespan=lifespan)

# --- 2. Health Check ---
@app.get("/")
def health_check(request: Request):
    """æª¢æŸ¥æœå‹™èˆ‡æ¨¡å‹ç‹€æ…‹"""
    models = request.app.state.models
    status = "ready" if models.get("cfp") is not None else "partial_service"
    
    return {
        "status": status, 
        "device": str(device),
        "loaded_models": [k for k, v in models.items() if v is not None]
    }

# --- 3. é æ¸¬å…¥å£ ---
# åˆ†é–‹è·¯ç”±ï¼Œè®“ API æ–‡ä»¶(Swagger)æ›´æ¸…æ™°

@app.post("/predict/cfp")
async def predict_cfp_endpoint(
    request: Request, 
    payload: PredictionRequest 
):
    models = request.app.state.models
    if models.get("cfp") is None:
        raise HTTPException(status_code=503, detail="CFP Model not initialized")

    try:
        # å‘¼å« Controller
        # æ³¨æ„ï¼šController å…§éƒ¨æœƒè² è²¬ç”¢ç”Ÿ CAM å’Œ YOLO åœ–ï¼Œä¸¦å›å‚³åŒ…å«é€™äº› URL çš„ dict
        result = await process_fundus_image(
            gcs_path=payload.image_gcs_path,
            model_cfp=models["cfp"],
            model_yolo=models["yolo"], 
            device=device
        )
        
        # å›å¡« request_id
        if payload.request_id:
            result["request_id"] = payload.request_id
            
        return JSONResponse(content=result)

    except Exception as e:
        # Log error...
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/predict/oct")
async def predict_oct_endpoint(
    request: Request, 
    payload: PredictionRequest
):
    return JSONResponse(content={
        "status": "pending",
        "message": "OCT inference not implemented",
        "source": payload.image_gcs_path
    })

if __name__ == "__main__":
    import uvicorn
    # æœ¬åœ°æ¸¬è©¦å•Ÿå‹•æŒ‡ä»¤
    uvicorn.run(app, host="0.0.0.0", port=8080)